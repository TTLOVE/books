"""
ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦» æ–‡æ¡£é«˜çº§åˆ†å—å¤„ç†
æ ¹æ®æŒ‡å®šç­–ç•¥è¿›è¡Œè¯­ä¹‰å®Œæ•´æ€§ã€æ¡ˆä¾‹ç‹¬ç«‹æ€§ã€å®éªŒæ•°æ®å®Œæ•´æ€§ã€ç†è®ºè®ºè¿°è¿è´¯æ€§çš„åˆ†å—
"""
import re
import json
from aiClient import get_ai_response
from bookModel import insert_data

def preprocess_yilian_textile(textile_src: str) -> str:
    """
    é¢„å¤„ç† Textile æºç  â†’ å¹²å‡€çš„æ–‡æœ¬å†…å®¹
    ä¿®å¤ï¼šè·¨è¡Œæ ‡é¢˜ã€h1./h2. æ ‡è®°ã€æ— ç©ºæ ¼é—®é¢˜
    """
    # ç§»é™¤ Textile è¡Œå†…æ ‡è®°ï¼ˆcalibre2, alt ç­‰æ— ç”¨å±æ€§ï¼‰
    text = re.sub(r'\(calibre\d+\)', '', textile_src)
    text = re.sub(r'\(alt\)', '', text)
    
    # å¤„ç† h1/h2/h3 æ ‡ç­¾ä¸­çš„ title# å±æ€§
    def remove_title_attr(match):
        h_part = match.group(1)      # h1, h2, æˆ– h3
        content = match.group(2)     # æ‹¬å·å†…çš„å®Œæ•´å†…å®¹
        # ç§»é™¤ title# å±æ€§éƒ¨åˆ†: title#...)
        clean_content = re.sub(r'title#[^)]*\)', '', content)
        return f'{h_part}({clean_content})'

    # åº”ç”¨æ ‡é¢˜å±æ€§ç§»é™¤
    text = re.sub(r'(h[1-3])\(([^)]*)\)', remove_title_attr, text)

    # åˆå¹¶è·¨è¡Œæ ‡é¢˜
    text = re.sub(
        r'(h[1-3]\([^)]*\)\.\s*[^\n]+)\n([^\n]+)',
        lambda m: m.group(1) + ' ' + m.group(2) if not re.match(r'(h[1-3]|div|images|---)', m.group(2).strip()) else m.group(0),
        text
    )

    # å°† h1./h2. â†’ æ¨¡æ‹Ÿ"ç« ""èŠ‚"æ ¼å¼
    def fix_heading(match):
        level = match.group(1)
        content = match.group(2).strip()
        
        # å°è¯•æå– PART æ•°å­—
        part_match = re.search(r'PART\s+(\d+)', content, re.IGNORECASE)
        if part_match:
            num = part_match.group(1)
            return f"ç¬¬{num}ç«  {content.replace('PART '+num, '').strip()}"
        
        # å°è¯•ç½—é©¬æ•°å­— â†’ èŠ‚
        if re.match(r'^[â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]+$', content):
            roman_map = {"â… ":"ä¸€", "â…¡":"äºŒ", "â…¢":"ä¸‰", "â…£":"å››", "â…¤":"äº”",
                         "â…¥":"å…­", "â…¦":"ä¸ƒ", "â…§":"å…«", "â…¨":"ä¹", "â…©":"å"}
            cn_num = ''.join(roman_map.get(c, c) for c in content)
            return f"ç¬¬{cn_num}èŠ‚ {content}"
        
        # æ™®é€šæ ‡é¢˜ï¼šè¡¥"ç« "å­—
        if level == "1":
            return f"ç« èŠ‚ {content}"
        else:
            return f"èŠ‚ {content}"

    text = re.sub(r'h([1-3])\([^)]*\)\.\s*(.+?)(?=\n|\Z)', fix_heading, text, flags=re.DOTALL)

    # ç¡®ä¿"ç« ""èŠ‚"åæœ‰ç©ºæ ¼
    text = re.sub(r'([ç« èŠ‚])([^ç« \n\s])', r'\1 \2', text)

    return text


def split_by_advanced_strategy(text: str, max_chunk_size: int = 900):
    """
    æ ¹æ®é«˜çº§ç­–ç•¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—ï¼š
    - è¯­ä¹‰å®Œæ•´æ€§åˆ†å—: ä¿è¯å‘å±•æè¿°çš„å®Œæ•´ä¸Šä¸‹æ–‡
    - æ¡ˆä¾‹ç‹¬ç«‹åˆ†å—: å„¿ç«¥ä¸ªæ¡ˆæè¿°å•ç‹¬æˆå—
    - å®éªŒæ•°æ®å®Œæ•´åˆ†å—: å®éªŒè®¾è®¡-è¿‡ç¨‹-ç»“æœä¿æŒä¸€ä½“
    - ç†è®ºè®ºè¿°è¿è´¯åˆ†å—: ç†è®ºæ¨å¯¼ä¿æŒé€»è¾‘è¿è´¯
    - æœ€å¤§é•¿åº¦: 800-1000 tokens
    """
    # è¯†åˆ«æ¡ˆä¾‹ç ”ç©¶æ ‡è®° - æ›´ç²¾ç¡®çš„æ¡ˆä¾‹è¯†åˆ«
    case_study_keywords = [
        'æ¡ˆä¾‹', 'ä¸ªæ¡ˆ', 'ç—…ä¾‹', 'æ‚£è€…', 'å„¿ç«¥', 'å°å­©', 'å°ç”·å­©', 'å°å¥³å­©', 
        'å­©å­', 'æ¥è®¿è€…', 'ç—…æ‚£', 'ç ”ç©¶å¯¹è±¡', 'å®éªŒå¯¹è±¡', 'è§‚å¯Ÿå¯¹è±¡', 
        'è®°å½•æ˜¾ç¤º', 'è§‚å¯Ÿç»“æœ', 'ä¸´åºŠè¡¨ç°', 'è¡¨ç°', 'è¡Œä¸º', 'ååº”',
        'æ‰˜å°¼', 'å‰å§†', 'é›ªè‰', 'é›·å‰'  # å…·ä½“æ¡ˆä¾‹åç§°
    ]
    
    # è¯†åˆ«ç« èŠ‚æ ‡é¢˜
    chapter_pattern = re.compile(
        r'^\s*(?:ç« èŠ‚|èŠ‚|ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚])\s+.+$',
        re.MULTILINE | re.UNICODE
    )
    
    # è¯†åˆ«ç†è®ºè®ºè¿°æ®µè½
    theory_keywords = [
        'ç†è®º', 'è®ºè¿°', 'æ¨å¯¼', 'å‡è®¾', 'æ¨¡å‹', 'æ¡†æ¶', 'æ¦‚å¿µ', 'å®šä¹‰', 
        'è§‚ç‚¹', 'è®¤ä¸º', 'æå‡º', 'å‘å±•', 'ç ”ç©¶è¡¨æ˜', 'ç ”ç©¶æŒ‡å‡º', 'æŒ‡å‡º',
        'ä¾æ‹ç†è®º', 'åˆ†ç¦»ç„¦è™‘', 'å®‰å…¨ä¾æ‹', 'ç„¦è™‘å‹ä¾æ‹'
    ]
    
    # è¯†åˆ«å®éªŒç›¸å…³éƒ¨åˆ†
    experiment_keywords = [
        'å®éªŒ', 'ç ”ç©¶', 'è§‚å¯Ÿ', 'æ•°æ®', 'ç»“æœ', 'æ–¹æ³•', 'è¿‡ç¨‹', 'è®¾è®¡', 
        'è°ƒæŸ¥', 'åˆ†æ', 'è®°å½•', 'æµ‹é‡', 'æµ‹è¯•', 'è¯„ä¼°', 'å®éªŒç»“æœ', 
        'ç ”ç©¶ç»“æœ', 'è§‚å¯Ÿç ”ç©¶', 'ä¸´åºŠè§‚å¯Ÿ', 'æ•°æ®åˆ†æ'
    ]
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    chunks = []
    current_chunk = ""
    
    # éå†æ‰€æœ‰æ®µè½
    i = 0
    while i < len(paragraphs):
        paragraph = paragraphs[i].strip()
        if not paragraph:
            i += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç« èŠ‚å¼€å§‹
        if chapter_pattern.match(paragraph):
            if current_chunk.strip():
                if len(current_chunk) > 50:  # é¿å…è¿‡å°çš„å—
                    chunks.append(current_chunk.strip())
                current_chunk = ""
            current_chunk += paragraph + "\n\n"
            i += 1
            continue
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«æ¡ˆä¾‹å†…å®¹
        is_case_study = any(keyword in paragraph for keyword in case_study_keywords)
        
        # æ¡ˆä¾‹ç‹¬ç«‹åˆ†å—ï¼šå„¿ç«¥ä¸ªæ¡ˆæè¿°å•ç‹¬æˆå—
        if is_case_study:
            # æ”¶é›†å®Œæ•´çš„æ¡ˆä¾‹æè¿°ï¼ˆå¯èƒ½è·¨è¶Šå¤šä¸ªæ®µè½ï¼‰
            case_chunk = paragraph
            j = i + 1
            
            # æŸ¥æ‰¾æ¡ˆä¾‹çš„è¾¹ç•Œ - ç›´åˆ°é‡åˆ°æ–°ä¸»é¢˜æˆ–ç« èŠ‚
            while j < len(paragraphs):
                next_para = paragraphs[j].strip()
                if not next_para:
                    j += 1
                    continue
                
                # æ£€æŸ¥ä¸‹ä¸€ä¸ªæ®µè½æ˜¯å¦ä»å±äºåŒä¸€æ¡ˆä¾‹
                next_is_case = any(keyword in next_para for keyword in case_study_keywords)
                next_is_chapter = chapter_pattern.match(next_para)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¡ˆä¾‹çš„å»¶ç»­ï¼ˆæœ‰æ¡ˆä¾‹ç›¸å…³å…³é”®è¯æˆ–ä¸æ¡ˆä¾‹ç›¸å…³çš„æè¿°ï¼‰
                is_case_continuation = (
                    next_is_case or 
                    any(keyword in next_para for keyword in ['ä»–', 'å¥¹', 'è¿™ä¸ªå­©å­', 'è¯¥æ¡ˆä¾‹', 'è¯¥ä¸ªæ¡ˆ', 'åœ¨è¿™ç§æƒ…å†µ'])
                )
                
                # å¦‚æœæ˜¯æ–°ç« èŠ‚æˆ–æ˜æ˜¾éæ¡ˆä¾‹å†…å®¹ï¼Œåˆ™åœæ­¢æ·»åŠ åˆ°å½“å‰æ¡ˆä¾‹
                if next_is_chapter or (not is_case_continuation and 
                                      all(kw not in next_para for kw in case_study_keywords + ['ä»–', 'å¥¹', 'è¿™ä¸ªå­©å­', 'è¯¥æ¡ˆä¾‹', 'è¯¥ä¸ªæ¡ˆ', 'åœ¨è¿™ç§æƒ…å†µ']) and
                                      len(next_para) > 100 and  # é•¿æ®µè½æ›´å¯èƒ½æ˜¯æ–°ä¸»é¢˜
                                      any(keyword in next_para for keyword in ['ç†è®º', 'ç ”ç©¶è¡¨æ˜', 'æ€»ç»“', 'ç»“è®º', 'åˆ†æ'])):
                    break
                
                # æ·»åŠ åˆ°æ¡ˆä¾‹å—ä¸­
                case_chunk += "\n\n" + next_para
                j += 1
            
            # ä¿å­˜æ¡ˆä¾‹å—
            if len(case_chunk) > 50:
                chunks.append(case_chunk.strip())
            
            # è·³è½¬åˆ°ä¸‹ä¸€ä¸ªéæ¡ˆä¾‹æ®µè½
            i = j
            continue
        
        # éæ¡ˆä¾‹å†…å®¹å¤„ç†
        paragraph_size = len(paragraph)
        current_size = len(current_chunk)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®éªŒç›¸å…³
        is_experiment = any(keyword in paragraph for keyword in experiment_keywords)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç†è®ºç›¸å…³
        is_theory = any(keyword in paragraph for keyword in theory_keywords)
        
        # è¯­ä¹‰å®Œæ•´æ€§ï¼šæ£€æµ‹å‘å±•æè¿°çš„ä¸Šä¸‹æ–‡
        is_development_context = any(keyword in paragraph for keyword in 
                                   ['å‘å±•', 'è¿‡ç¨‹', 'é˜¶æ®µ', 'æ¼”å˜', 'æ¼”è¿›', 'æˆé•¿', 
                                    'å˜åŒ–', 'è¿›å±•', 'å½±å“', 'ä½œç”¨', 'æ•ˆåº”', 'å…³ç³»',
                                    'ä¾æ‹', 'æ¯å©´', 'åˆ†ç¦»', 'ç„¦è™‘', 'ææƒ§', 'å®‰å…¨'])
        
        # å®éªŒæ•°æ®å®Œæ•´åˆ†å—ï¼šä¿æŒå®éªŒè®¾è®¡-è¿‡ç¨‹-ç»“æœä¿æŒä¸€ä½“
        if is_experiment:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¼€å§‹æ–°çš„å®éªŒå—
            has_experiment_content = any(keyword in current_chunk for keyword in experiment_keywords)
            
            # å¦‚æœå½“å‰å—åŒ…å«éå®éªŒå†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰å—ï¼Œç„¶åå¼€å§‹æ–°å®éªŒå—
            if current_chunk.strip() and not has_experiment_content:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
            # å¦‚æœå½“å‰å—å·²ç»åœ¨åŒ…å«å®éªŒå†…å®¹ï¼Œç»§ç»­æ·»åŠ 
            elif current_chunk.strip() and has_experiment_content:
                # æ£€æŸ¥å½“å‰å®éªŒå—æ˜¯å¦è¿‡å¤§
                if current_size + paragraph_size > max_chunk_size:
                    # å¦‚æœå®éªŒç›¸å…³çš„æ®µè½ï¼Œå³ä½¿è¶…è¿‡é•¿åº¦ä¹Ÿå°½é‡ä¿æŒåœ¨ä¸€èµ·
                    if any(keyword in paragraph for keyword in ['æ–¹æ³•', 'ç»“æœ', 'æ•°æ®', 'è§‚å¯Ÿ']):
                        current_chunk += paragraph + "\n\n"
                    else:
                        chunks.append(current_chunk.strip())
                        current_chunk = paragraph + "\n\n"
                else:
                    current_chunk += paragraph + "\n\n"
            # å¦‚æœæ²¡æœ‰å½“å‰å—ï¼Œå¼€å§‹æ–°çš„å®éªŒå—
            else:
                current_chunk = paragraph + "\n\n"
        
        # ç†è®ºè®ºè¿°è¿è´¯åˆ†å—ï¼šä¿æŒé€»è¾‘æ¨å¯¼è¿è´¯
        elif is_theory or is_development_context:
            # æ£€æŸ¥å½“å‰å—æ˜¯å¦åŒ…å«ç†è®ºæˆ–å‘å±•å†…å®¹
            has_theory_content = any(keyword in current_chunk for keyword in theory_keywords)
            has_dev_content = any(keyword in current_chunk for keyword in 
                                 ['å‘å±•', 'è¿‡ç¨‹', 'é˜¶æ®µ', 'æ¼”å˜', 'æ¼”è¿›', 'æˆé•¿', 
                                  'å˜åŒ–', 'è¿›å±•', 'å½±å“', 'ä½œç”¨', 'æ•ˆåº”', 'å…³ç³»',
                                  'ä¾æ‹', 'æ¯å©´', 'åˆ†ç¦»', 'ç„¦è™‘', 'ææƒ§', 'å®‰å…¨'])
            
            should_continue_theory = has_theory_content or has_dev_content
            
            # å¦‚æœå½“å‰å—åŒ…å«éç†è®º/éå‘å±•å†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰å—ï¼Œç„¶åå¼€å§‹æ–°ç†è®ºå—
            if current_chunk.strip() and not should_continue_theory:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
            # å¦‚æœå½“å‰å—å·²ç»åœ¨åŒ…å«ç†è®º/å‘å±•å†…å®¹ï¼Œç»§ç»­æ·»åŠ 
            elif current_chunk.strip() and should_continue_theory:
                # å¯¹äºç†è®ºå†…å®¹ï¼Œå³ä½¿ç•¥å¾®è¶…è¿‡é•¿åº¦é™åˆ¶ä¹Ÿå°½é‡ä¿æŒé€»è¾‘è¿è´¯
                if current_size + paragraph_size > max_chunk_size:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç†è®ºçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¿æŒè¿è´¯æ€§
                    if any(keyword in paragraph for keyword in ['å› æ­¤', 'æ‰€ä»¥', 'ç”±æ­¤', 'è¿™è¡¨æ˜', 'å¯ä»¥çœ‹å‡º', 'åŸºäº']):
                        current_chunk += paragraph + "\n\n"
                    else:
                        chunks.append(current_chunk.strip())
                        current_chunk = paragraph + "\n\n"
                else:
                    current_chunk += paragraph + "\n\n"
            # å¦‚æœæ²¡æœ‰å½“å‰å—ï¼Œå¼€å§‹æ–°çš„ç†è®ºå—
            else:
                current_chunk = paragraph + "\n\n"
        
        # æ™®é€šå†…å®¹å¤„ç†ï¼šæŒ‰é•¿åº¦æ§åˆ¶åˆ†å—
        else:
            if current_size + paragraph_size > max_chunk_size:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
            else:
                current_chunk += paragraph + "\n\n"
        
        i += 1
    
    # æ·»åŠ æœ€åä¸€å—
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # è¿›ä¸€æ­¥æ‹†åˆ†è¿‡é•¿çš„å—ï¼Œä½†è¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_chunk_size:
            # è¯†åˆ«å—çš„ä¸»è¦ç±»å‹ä»¥å†³å®šå¦‚ä½•æ‹†åˆ†
            is_case_chunk = any(keyword in chunk for keyword in case_study_keywords)
            is_exp_chunk = any(keyword in chunk for keyword in experiment_keywords)
            is_theory_chunk = any(keyword in chunk for keyword in theory_keywords)
            is_dev_chunk = any(keyword in chunk for keyword in 
                              ['å‘å±•', 'è¿‡ç¨‹', 'é˜¶æ®µ', 'æ¼”å˜', 'æ¼”è¿›', 'æˆé•¿', 
                               'å˜åŒ–', 'è¿›å±•', 'å½±å“', 'ä½œç”¨', 'æ•ˆåº”', 'å…³ç³»'])
            
            if is_case_chunk or is_exp_chunk:
                # æ¡ˆä¾‹æˆ–å®éªŒå—é€šå¸¸ä¸æ‹†åˆ†ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä¿æŒå®Œæ•´æ€§
                # ä½†å¦‚æœæ˜¯ç‰¹åˆ«é•¿çš„æ¡ˆä¾‹/å®éªŒï¼Œå¯ä»¥é€‚å½“æ‹†åˆ†
                if len(chunk) > max_chunk_size * 2:  # å¦‚æœæ˜¯ä¸¤å€é•¿åº¦ä»¥ä¸Š
                    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', chunk)
                    temp_chunk = ""
                    
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if not sentence:
                            continue
                        
                        # å¯¹äºæ¡ˆä¾‹å’Œå®éªŒï¼Œå³ä½¿æ˜¯é•¿å—ä¹Ÿè¦åœ¨åˆé€‚çš„åœ°æ–¹æ‹†åˆ†
                        if len(temp_chunk + sentence) <= max_chunk_size:
                            temp_chunk += sentence + ". "
                        else:
                            if temp_chunk.strip():
                                final_chunks.append(temp_chunk.strip())
                            temp_chunk = sentence + ". "
                    
                    if temp_chunk.strip():
                        final_chunks.append(temp_chunk.strip())
                else:
                    # æ™®é€šé•¿åº¦çš„æ¡ˆä¾‹/å®éªŒå—ä¿æŒå®Œæ•´
                    final_chunks.append(chunk)
            else:
                # å¯¹äºéæ¡ˆä¾‹/éå®éªŒå—ï¼ŒæŒ‰å¥å­æ‹†åˆ†ä½†ä¿æŒæœ€å¤§é•¿åº¦
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s*', chunk)
                temp_chunk = ""
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    if len(temp_chunk + sentence) <= max_chunk_size:
                        temp_chunk += sentence + ". "
                    else:
                        if temp_chunk.strip():
                            final_chunks.append(temp_chunk.strip())
                        temp_chunk = sentence + ". "
                
                if temp_chunk.strip():
                    final_chunks.append(temp_chunk.strip())
        else:
            final_chunks.append(chunk)
    
    # è¿‡æ»¤æ‰å¤ªå°çš„å—
    final_chunks = [chunk for chunk in final_chunks if len(chunk.strip()) > 50]
    
    return final_chunks


def split_chapters_robust(text: str):
    """å¢å¼ºç‰ˆç« èŠ‚åˆ†å‰²å‡½æ•°"""
    lines = text.splitlines()
    chapters = []
    current_title = ""
    current_content_lines = []

    # å…è®¸çš„æ ‡é¢˜æ¨¡å¼
    title_pattern = re.compile(
        r'^\s*(?:ç« èŠ‚|èŠ‚|ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚])\s+.+$',
        re.UNICODE
    )

    for line in lines:
        stripped = line.strip()
        if not stripped:
            current_content_lines.append(line)
            continue

        if title_pattern.match(line):
            # ä¿å­˜ä¸Šä¸€ç« 
            if current_title or current_content_lines:
                chapters.append({
                    "title": current_title,
                    "content": "\n".join(current_content_lines).strip()
                })
            # æ–°ç« èŠ‚
            clean_title = re.sub(r'^\s*(?:ç« èŠ‚|èŠ‚)\s*', '', stripped)
            clean_title = re.sub(r'^ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚]\s*', '', clean_title)
            current_title = clean_title.strip()
            current_content_lines = []
        else:
            current_content_lines.append(line)

    # ä¿å­˜æœ€åä¸€ç« 
    if current_title or current_content_lines:
        chapters.append({
            "title": current_title or "å‰è¨€",
            "content": "\n".join(current_content_lines).strip()
        })

    return chapters


def chunk_yilian_advanced():
    """
    å¯¹ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦»è¿›è¡Œé«˜çº§åˆ†å—å¤„ç†
    """
    # è¯»å– textile æ–‡ä»¶
    with open("./textiles/ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦».textile", encoding="utf-8") as f:
        textile_src = f.read()

    # é¢„å¤„ç† + åˆ‡åˆ†ç« èŠ‚
    clean_text = preprocess_yilian_textile(textile_src)
    chapters = split_chapters_robust(clean_text)

    print(f"âœ… æˆåŠŸåˆ‡åˆ† {len(chapters)} ç« ")
    
    all_chunks_with_titles = []
    
    for i, ch in enumerate(chapters, 1):
        print(f"[{i}] {ch['title']}... {len(ch['content'])}å­—")
        
        # å¯¹æ¯ç« å†…å®¹è¿›è¡Œé«˜çº§ç­–ç•¥åˆ†å—
        chunks = split_by_advanced_strategy(ch['content'])
        
        print(f"  â””â”€â”€ åˆ†å‰²ä¸º {len(chunks)} ä¸ªè¯­ä¹‰å—")
        
        # ä¸ºæ¯ä¸ªå—åˆ›å»ºæ ‡é¢˜ï¼ˆåŒ…å«ç« èŠ‚ä¿¡æ¯ï¼‰
        for j, chunk in enumerate(chunks):
            chunk_title = f"{ch['title']} - ç¬¬{j+1}å—"
            all_chunks_with_titles.append({
                "title": chunk_title,
                "content": chunk,
                "original_chapter": ch['title']
            })
    
    print(f"ğŸ“¦ æ€»å…±ç”Ÿæˆ {len(all_chunks_with_titles)} ä¸ªè¯­ä¹‰å—")
    
    # å¤„ç†æ¯ä¸ªå—ï¼Œè·å–AIå“åº”å¹¶æ’å…¥æ•°æ®åº“
    for idx, chunk_data in enumerate(all_chunks_with_titles, 1):
        print(f"\nå¤„ç†å— {idx}/{len(all_chunks_with_titles)}: {chunk_data['title'][:50]}...")
        
        try:
            result_data = get_ai_response(chunk_data["content"])
            if len(result_data) == 0:
                print("  âš ï¸ result_data is empty")
                continue

            batch_data = []
            if isinstance(result_data, list):
                # æ–°å“åº”æ ¼å¼
                for item in result_data:
                    content = item.get('content', '')
                    relevant_age_group = item.get('relevant_age_group', '')
                    relevant_domain = item.get('relevant_domain', 'å…¶ä»–')
                    tags = item.get('tags', [])
                    
                    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                    if isinstance(relevant_age_group, str):
                        relevant_age_group = [relevant_age_group] if relevant_age_group else []
                    if isinstance(relevant_domain, str):
                        relevant_domain = [relevant_domain] if relevant_domain else []
                    
                    tags_json = json.dumps(tags, ensure_ascii=False)                                                 
                    categories_json = json.dumps(relevant_domain, ensure_ascii=False)                                         
                    ages_json = json.dumps(relevant_age_group, ensure_ascii=False)                                                     
                    summary = content
                    
                    batch_data.append(("ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦»", chunk_data["title"], summary, 
                                     chunk_data["original_chapter"], tags_json, categories_json, ages_json))
            else:
                # å…¼å®¹æ—§æ ¼å¼
                tags_json = json.dumps(result_data.get('points', []), ensure_ascii=False)                                                 
                categories_json = json.dumps(result_data.get('categories', []), ensure_ascii=False)                                         
                ages_json = json.dumps(result_data.get('ages', []), ensure_ascii=False)                                                     
                summary = result_data.get('summary', '')                                                                
                batch_data.append(("ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦»", chunk_data["title"], summary, 
                                 chunk_data["original_chapter"], tags_json, categories_json, ages_json))

        except json.JSONDecodeError as e:
            print(f"  âŒ JSON è§£æé”™è¯¯: {e}")
            continue
        except Exception as e:
            print(f"  âŒ å¤„ç†å— {idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue

        # æ‰¹é‡æ’å…¥æ•°æ®
        if batch_data:
            from bookModel import batch_insert_data
            success = batch_insert_data(batch_data)
            if success:
                print(f"  âœ… æ‰¹é‡æ’å…¥ {len(batch_data)} æ¡è®°å½•æˆåŠŸ")
            else:
                print("  âŒ æ‰¹é‡æ’å…¥å¤±è´¥")
        else:
            print("  âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®éœ€è¦æ’å…¥")


def yilian_simple_chunk():
    """
    ç®€å•åˆ†å—ç‰ˆæœ¬ï¼Œç”¨äºæµ‹è¯•
    """
    # è¯»å– textile æ–‡ä»¶
    with open("./textiles/ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦».textile", encoding="utf-8") as f:
        textile_src = f.read()

    # é¢„å¤„ç† + åˆ‡åˆ†ç« èŠ‚
    clean_text = preprocess_yilian_textile(textile_src)
    chapters = split_chapters_robust(clean_text)

    print(f"âœ… æˆåŠŸåˆ‡åˆ† {len(chapters)} ç« ")
    
    for i, ch in enumerate(chapters, 1):
        print(f"[{i}] {ch['title']}... {len(ch['content'])}å­—")
        
        # ä½¿ç”¨é«˜çº§ç­–ç•¥åˆ†å—
        chunks = split_by_advanced_strategy(ch['content'])
        
        print(f"  â””â”€â”€ åˆ†å‰²ä¸º {len(chunks)} ä¸ªè¯­ä¹‰å—")
        
        for j, chunk in enumerate(chunks):
            print(f"    å— {j+1}: {len(chunk)} å­—ç¬¦")
            
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨AIå¤„ç†å’Œæ•°æ®åº“æ’å…¥
            # result_data = get_ai_response(chunk_content)
            # insert_data("ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦»", f"{ch['title']}-å—{j+1}", ...)


if __name__ == "__main__":
    print("å¼€å§‹å¤„ç† ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦» çš„é«˜çº§åˆ†å—...")
    chunk_yilian_advanced()