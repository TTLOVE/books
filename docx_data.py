#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
import os
import json
from aiClient import get_ai_response
import asyncio
from concurrent.futures import ThreadPoolExecutor

def split_chapters_by_pattern(text):
    """
    æ ¹æ®"ç¬¬ä¸€ç« \näººç±»æ‚²ä¼¤çš„åŸå‹"æ ¼å¼åˆ‡åˆ†ç« èŠ‚
    ç« èŠ‚ç¼–å·å’Œç« èŠ‚åç§°åˆ†åˆ«åœ¨ä¸åŒè¡Œ
    """
    lines = text.splitlines()
    chapters = []
    current_title = ""
    current_content_lines = []

    # åŒ¹é…ç« èŠ‚ç¼–å·æ ¼å¼ï¼Œå¦‚"ç¬¬ä¸€ç« "ã€"ç¬¬äºŒç« "ç­‰ï¼ˆå•ç‹¬ä¸€è¡Œï¼‰
    chapter_number_pattern = re.compile(
        r'^\s*ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+[ç« èŠ‚]\s*$',
        re.UNICODE
    )

    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        if chapter_number_pattern.match(line):
            # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜
            if i + 1 < len(lines):
                next_line = lines[i + 1].strip()
                # å¦‚æœä¸‹ä¸€è¡Œéç©ºä¸”ä¸æ˜¯å¦ä¸€ä¸ªç« èŠ‚ç¼–å·ï¼Œåˆ™è®¤ä¸ºæ˜¯ç« èŠ‚æ ‡é¢˜
                if next_line and not chapter_number_pattern.match(next_line):
                    # ä¿å­˜ä¸Šä¸€ç« èŠ‚ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
                    if current_title or current_content_lines:
                        chapters.append({
                            "title": current_title,
                            "content": "\n".join(current_content_lines).strip()
                        })
                    
                    # è®¾ç½®æ–°ç« èŠ‚æ ‡é¢˜ï¼Œè·³è¿‡ç« èŠ‚ç¼–å·å’Œæ ‡é¢˜ä¸¤è¡Œ
                    current_title = next_line
                    i += 2  # è·³è¿‡ç« èŠ‚ç¼–å·è¡Œå’Œæ ‡é¢˜è¡Œ
                    current_content_lines = []
                    continue
                else:
                    # å¦‚æœä¸‹ä¸€è¡Œæ˜¯å¦ä¸€ä¸ªç« èŠ‚ç¼–å·æˆ–ä¸ºç©ºï¼Œåˆ™å½“å‰è¡Œå¯èƒ½åªæ˜¯æ™®é€šå†…å®¹
                    current_content_lines.append(lines[i])
            else:
                # å¦‚æœå½“å‰è¡Œæ˜¯æœ€åä¸€æ¡ä¸”æ²¡æœ‰ä¸‹ä¸€è¡Œæ ‡é¢˜ï¼Œåˆ™å¯èƒ½åªæ˜¯æ™®é€šå†…å®¹
                current_content_lines.append(lines[i])
            i += 1
        else:
            # ä¸æ˜¯ç« èŠ‚ç¼–å·è¡Œï¼Œæ·»åŠ åˆ°å½“å‰å†…å®¹
            current_content_lines.append(lines[i])
            i += 1

    # æ·»åŠ æœ€åä¸€ç« èŠ‚
    if current_title or current_content_lines:
        chapters.append({
            "title": current_title or "å‰è¨€",
            "content": "\n".join(current_content_lines).strip()
        })

    return chapters


def extract_text_from_docx(file_path):
    """
    ä» DOCX æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
    """
    try:
        import zipfile
        import xml.etree.ElementTree as ET
        
        with zipfile.ZipFile(file_path, 'r') as docx_zip:
            # è¯»å–ä¸»æ–‡æ¡£å†…å®¹
            content_xml = docx_zip.read('word/document.xml')
            tree = ET.fromstring(content_xml)
            
            # å®šä¹‰å‘½åç©ºé—´
            namespaces = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
            }
            
            # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬
            paragraphs = []
            for paragraph in tree.iterfind('.//w:p', namespaces):
                texts = []
                for text_elem in paragraph.iterfind('.//w:t', namespaces):
                    if text_elem is not None and text_elem.text:
                        texts.append(text_elem.text)
                if texts:
                    paragraphs.append(''.join(texts))
            
            return '\n'.join(paragraphs)
    except Exception as e:
        print(f"âŒ æ— æ³•ä» DOCX æ–‡ä»¶æå–æ–‡æœ¬: {e}")
        return None

def get_title_and_page(title):
    """
    è·å–æ ‡é¢˜å’Œé¡µæ•°
    """
    page = 0
    # åŒ¹é…æ ‡é¢˜åç§°å’Œé¡µæ•°ä¿¡æ¯,æ ‡é¢˜å¯èƒ½ä¸ºï¼šäººç±»æ‚²ä¼¤çš„åŸå‹   003ï¼Œéœ€è¦è·å–æ ‡é¢˜åç§°å’Œé¡µæ•°ä¿¡æ¯
    parts = re.split(r'\s+', title.strip(), maxsplit=1)
    if len(parts) > 1:
        title = parts[0]
        page = parts[1]
        page = page.replace('.', '').replace('/', '1')

        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ•°å­—
        if re.search(r'\d', page):
            page = int(page)
        else:
            title = title + " " + page
            page = 0
    else:
        title = re.sub(r'\d+', '', title)

    return title, page

def process_yilian_docx():
    """
    å¤„ç†ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦».docx æ–‡ä»¶å¹¶è¾“å‡ºåˆ‡åˆ†ä¿¡æ¯
    """
    docx_path = "./ä¾æ‹ä¸‰éƒ¨æ›²â€¢ç¬¬äºŒå·åˆ†ç¦».docx"
    
    print(f"ğŸ“– æ­£åœ¨å¤„ç†æ–‡ä»¶: {docx_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(docx_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {docx_path}")
        return
    
    # ä» DOCX æ–‡ä»¶æå–æ–‡æœ¬
    print("ğŸ“– æ­£åœ¨ä» DOCX æ–‡ä»¶ä¸­æå–æ–‡æœ¬...")
    text_content = extract_text_from_docx(docx_path)
    
    if text_content is None:
        print("âŒ æ— æ³•æå– DOCX æ–‡ä»¶å†…å®¹")
        return
    
    print(f"âœ… æˆåŠŸæå–æ–‡æœ¬ï¼Œå…± {len(text_content)} ä¸ªå­—ç¬¦")
    
    # æŒ‰ç« èŠ‚æ¨¡å¼åˆ‡åˆ†æ–‡æ¡£
    print("âœ‚ï¸ æ­£åœ¨æŒ‰'ç¬¬ä¸€ç« \\näººç±»æ‚²ä¼¤çš„åŸå‹'æ ¼å¼åˆ‡åˆ†ç« èŠ‚...")
    chapters = split_chapters_by_pattern(text_content)

    print(f"âœ… æˆåŠŸè¯†åˆ«å‡º {len(chapters)} ä¸ªç« èŠ‚")
    print("=" * 80)
    print("ğŸ“‹ ç« èŠ‚åˆ‡åˆ†ç»“æœ:")
    print("=" * 80)
    
    group_chapters = {}
    chapter_index = 0
    current_chapter_index = 0
    current_page = 0
    content = ""
    
    # Store the tasks to be run concurrently
    ai_tasks = []
    
    for i, chapter in enumerate(chapters, 1):
        if chapter['title'] == "":
            continue

        title, page = get_title_and_page(chapter['title'])
        if title not in group_chapters:
            chapter_index += 1
            group_chapters[title] = {
                "title":  f"ç¬¬ {chapter_index} ç« : {title}",
            }

        if page > 0:
            current_page = page
        else:
            current_page = current_page + 2

        title = group_chapters[title]
        print(title['title'], current_page)

        # ç« å˜åŒ–,åˆ™å…ˆå°†AIä»»åŠ¡æ·»åŠ åˆ°åˆ—è¡¨,ç„¶åæ¸…ç©ºå†…å®¹
        if current_chapter_index != chapter_index and current_chapter_index != 0:
            print(f"ç« å˜åŒ–,å‡†å¤‡å‘é€å†…å®¹åˆ°AI: {current_page}")
            if current_page >= 95:
                ai_tasks.append((chapter_index - 1, content))
            content = chapter['content']
        else:
            content = content + chapter['content']

        current_chapter_index = chapter_index
    
    # Add the last content if there's any
    if content and current_chapter_index > 0:
        print(f"å‡†å¤‡å‘é€æœ€åä¸€éƒ¨åˆ†å†…å®¹åˆ°AI: {current_page}")
        if current_page >= 95:
            ai_tasks.append((current_chapter_index, content))
    
    # Execute all AI tasks concurrently
    if ai_tasks:
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(ai_tasks)} ä¸ªAIä»»åŠ¡...")
        with ThreadPoolExecutor() as executor:
            # Submit all tasks
            futures = [executor.submit(get_ai_response_and_insert_data, chapter_idx, cont) 
                      for chapter_idx, cont in ai_tasks]
            
            # Wait for all tasks to complete
            for future in futures:
                future.result()  # This ensures any exceptions are raised
        print("âœ… æ‰€æœ‰AIä»»åŠ¡å®Œæˆ")

    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š æ€»è®¡: {len(chapters)} ä¸ªç« èŠ‚")
    total_chars = sum(len(ch['content']) for ch in chapters)
    print(f"æ€»å†…å®¹å­—ç¬¦æ•°: {total_chars}")
    
    # print("\nâ€”" * 80)
    # print("æ‰€æœ‰ç« èŠ‚æ ‡é¢˜åˆ—è¡¨:")
    # print("â€”" * 80)
    # for i, chapter in enumerate(chapters, 1):
    #     print(f"{i:3d}. {chapter['title']}")

def get_ai_response_and_insert_data(captcher: int, content):
    try:
        result_data = get_ai_response(captcher, content)
        if len(result_data) == 0:
            print("result_data is empty")
            return

        # print("è§£é‡Šåçš„æ•°æ®:", result_data)
        batch_data = []
        
        # """ {
        #     "knowledge_id": "K_ATT_1_0-72æœˆ_045",
        #     "relevant_age_group": ["0-3æœˆ", "3-6æœˆ", "6-9æœˆ", "9-12æœˆ", "12-15æœˆ", "15-18æœˆ", "18-24æœˆ", "24-30æœˆ", "30-36æœˆ", "36-48æœˆ", "48-60æœˆ", "60-72æœˆ"],
        #     "original_text_segment": "å¯¹äºä¸€ä¸ªå¹¼å„¿æˆ–è€…æˆäººæ¥è¯´ï¼Œä»–æ˜¯å¤„äºä¸€ç§å®‰å…¨çŠ¶æ€ã€ç„¦è™‘çŠ¶æ€è¿˜æ˜¯å¿§éƒçŠ¶æ€ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å–å†³äºä»–çš„é¦–è¦ä¾æ‹å¯¹è±¡æ˜¯å¦æ˜¯å¯æ¥è¿‘çš„ã€å¯åˆ°è¾¾çš„å’Œæœ‰ååº”çš„ã€‚",
        #     "text_location": "ç¬¬1ç« ç¬¬24é¡µç¬¬2æ®µ",
        #     "extraction_context": "è®ºè¿°ä¾æ‹å¯¹è±¡å¯æ¥è¿‘æ€§å¯¹æƒ…ç»ªçŠ¶æ€çš„æ ¸å¿ƒå½±å“",
        #     "inference_level": "ç†è®ºæ¨è®º",
        #     "extraction_basis": "ä»ç†è®ºè®ºè¿°ä¸­æ¨ç†å‡ºä¾æ‹å¯¹è±¡å¯æ¥è¿‘æ€§å’Œååº”æ€§æ˜¯å†³å®šå¹¼å„¿æƒ…ç»ªçŠ¶æ€çš„å…³é”®å› ç´ ï¼Œé€‚ç”¨äºæ•´ä¸ªå„¿ç«¥æœŸ",
        #     "confidence_level": "é«˜ç½®ä¿¡åº¦",
        #     "original_age_reference": "å¹¼å„¿æˆ–æˆäºº",
        #     "content_summary": "ä¾æ‹å¯¹è±¡å¯æ¥è¿‘æ€§å’Œååº”æ€§æ˜¯å†³å®šå¹¼å„¿æƒ…ç»ªçŠ¶æ€ï¼ˆå®‰å…¨ã€ç„¦è™‘ã€å¿§éƒï¼‰çš„å…³é”®å› ç´ ",
        #     "source_document": "ä¾æ‹ä¸‰éƒ¨æ›²ãƒ»ç¬¬äºŒå·åˆ†ç¦»",
        #     "development_aspect": ["è¡Œä¸ºè¡¨ç°", "å½±å“å› ç´ "],
        #     "domain_category": ["ç¤¾ä¼šè¡Œä¸º"],
        #     "sensitive_period": {
        #     "category": "äººé™…",
        #     "manifestation": "å¯¹ä¾æ‹å¯¹è±¡å¯æ¥è¿‘æ€§æ•æ„Ÿï¼Œå½±å“æ•´ä½“æƒ…ç»ªå¥åº·"
        #     },
        #     "intelligence_development": {
        #     "category": "å†…çœ",
        #     "manifestation": "èƒ½æ„ŸçŸ¥ä¾æ‹å¯¹è±¡å¯ç”¨æ€§å¹¶è°ƒèŠ‚æƒ…ç»ª"
        #     },
        #     "evidence_quality": "å¼ºè¯æ®",
        #     "extraction_notes": "åŸºäºç†è®ºæ€»ç»“ï¼Œå¹´é¾„èŒƒå›´ä»å¹¼å„¿æœŸæ‰©å±•åˆ°æ•´ä¸ªå„¿ç«¥æœŸ"
        # }"""

        # Process the new response format: result_data is a list of objects
        for item in result_data:
            relevant_age_group = item.get("relevant_age_group", "")
            original_text_segment = item.get("original_text_segment", "")
            text_location = item.get("text_location", "")
            extraction_context = item.get("extraction_context", "")
            inference_level = item.get("inference_level", "")
            extraction_basis = item.get("extraction_basis", "")
            confidence_level = item.get("confidence_level", "")
            original_age_reference = item.get("original_age_reference", "")
            content_summary = item.get("content_summary", "")
            source_document = item.get("source_document", "")
            development_aspect = item.get("development_aspect", [])
            domain_category = item.get("domain_category", [])
            sensitive_period = item.get("sensitive_period", {})
            intelligence_development = item.get("intelligence_development", {})
            evidence_quality = item.get("evidence_quality", "")
            extraction_notes = item.get("extraction_notes", "")

            # Convert single values to lists if needed
            if isinstance(relevant_age_group, str):
                relevant_age_group = (
                    [relevant_age_group] if relevant_age_group else []
                )
            if isinstance(development_aspect, str):
                development_aspect = [development_aspect] if development_aspect else []
            if isinstance(domain_category, str):
                domain_category = [domain_category] if domain_category else []
            if isinstance(sensitive_period, str):
                sensitive_period = [sensitive_period] if sensitive_period else []
            if isinstance(intelligence_development, str):
                intelligence_development = [intelligence_development] if intelligence_development else []

            relevant_age_group = json.dumps(relevant_age_group, ensure_ascii=False)
            development_aspect = json.dumps(development_aspect, ensure_ascii=False)
            domain_category = json.dumps(domain_category, ensure_ascii=False)
            sensitive_period = json.dumps(sensitive_period, ensure_ascii=False)
            intelligence_development = json.dumps(intelligence_development, ensure_ascii=False)

            # Add data to batch list instead of inserting individually
            batch_data.append(
                (
                    relevant_age_group,
                    original_text_segment,
                    text_location,
                    extraction_context,
                    inference_level,
                    extraction_basis,
                    confidence_level,
                    original_age_reference,
                    content_summary,
                    source_document,
                    development_aspect,
                    domain_category,
                    sensitive_period,
                    intelligence_development,
                    evidence_quality,
                    extraction_notes,
                )
            )

    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æé”™è¯¯ for chapter: {e}")
        print(f"AI response was: {result_data}")
        return
    except Exception as e:
        print(f"âŒ å¤„ç†ç« èŠ‚ {captcher} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return

    # Perform batch insertion
    if batch_data:
        from bookModel import batch_insert_knowledge

        # print("batch_data: ", batch_data)
        success = batch_insert_knowledge(batch_data)
        if success:
            print(f"âœ… æ‰¹é‡æ’å…¥ {len(batch_data)} æ¡è®°å½•æˆåŠŸ")
        else:
            print("âŒ æ‰¹é‡æ’å…¥å¤±è´¥")
    else:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®éœ€è¦æ’å…¥")


if __name__ == "__main__":
    process_yilian_docx()